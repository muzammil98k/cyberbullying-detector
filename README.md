# AI-Augmented System for Identifying Online Harassment and Cyberbullying

## 💡 Project Overview
This project aims to build an AI system capable of detecting toxic, hateful, or offensive language in user-generated content. It focuses on identifying different forms of online harassment using NLP and machine learning models.

## 📂 Dataset
We are using the [Jigsaw Toxic Comment Classification Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). The dataset contains Wikipedia comments labeled for multiple types of toxicity.

## 🧠 Technologies Used
- Python
- Scikit-learn
- NLTK
- Pandas / NumPy
- Matplotlib / Seaborn
- (Optional) Transformers (BERT), Keras, Gradio

## 🏗️ Project Structure
cyberbullying-detector/
├── data/
├── notebooks/
├── models/
├── src/
│   ├── preprocessing.py
│   ├── train.py
│   └── predict.py
├── README.md
├── requirements.txt
└── app.py

## ✅ Goals
- Preprocess and analyze text data
- Train classification models
- Build a simple UI to detect cyberbullying
- Evaluate and explain model predictions

## 📌 Status
Day 1 complete: Project setup, dataset selected, and initial files created.
